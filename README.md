# SMAI_Implementation

This repository contains a series of comprehensive implementations of statistical modeling and machine learning algorithms, developed as part of a course on **Statistical Methods in AI**.

For brevity, data sources have not been included. Each part of the project includes extensive hyperparameter tuning, data exploration, and model optimizations wherever possible, emphasizing the importance of performance efficiency and practical applicability.

## Table of Contents

1. [Part 1: k-Nearest Neighbours and Decision Trees](#part-1-k-nearest-neighbours-and-decision-trees)
2. [Part 2: PCA, Gaussian Mixture Models, and Clustering](#part-2-pca-gaussian-mixture-models-and-clustering)
3. [Part 3: Linear Models, MLPs, CNNs, and Autoencoders](#part-3-linear-models-mlps-cnns-and-autoencoders)
4. [Part 4: Ensemble Methods](#part-4-ensemble-methods)
5. [Part 5: Kernel Density Estimation and Hidden Markov Models](#part-5-kernel-density-estimation-and-hidden-markov-models)

## Part 1: k-Nearest Neighbours and Decision Trees

- **k-Nearest Neighbours (k-NN)**: Implemented the k-NN algorithm from scratch, focusing on optimizing the algorithm through vectorization techniques to enhance computational efficiency. 
  
- **Decision Trees**: Explored the powerset and multi-output formulations of classification using decision trees. Explored the flexibility of decision trees in handling complex classification problems with multiple outputs, providing insights into the interpretability and structure of tree-based models.

## Part 2: PCA, Gaussian Mixture Models, and Clustering

- **Principal Component Analysis (PCA)**: Implemented PCA from scratch, emphasizing its role in dimensionality reduction and feature extraction. 
  
- **Gaussian Mixture Models (GMMs)**: Developed the Expectation-Maximization algorithm (A probabilistic algorithm) for GMMs from the ground up.
  
- **Hierarchical Clustering**: Conducted ablation studies on hierarchical clustering, providing insights into the clustering process and its sensitivity to different linkage criteria and distance metrics. 
## Part 3: Linear Models, MLPs, CNNs, and Autoencoders

- **Multinomial Linear and Logistic Regression**: Implemented these foundational models from scratch, focusing on their application to multi-class classification problems.
  
- **Multilayer Perceptrons (MLPs)**: Built MLPs from the ground up, exploring the role of hidden layers and activation functions in deep learning. 
  
- **Convolutional Neural Networks (CNNs) and Autoencoders**: Leveraged inbuilt Torch modules to implement CNNs and autoencoders, focusing on their application to image processing tasks. This work highlights the importance of convolutional operations in feature extraction and the role of autoencoders in unsupervised learning and dimensionality reduction.

## Part 4: Ensemble Methods

- **Bagging and Stacking**: Implemented Bagging and Stacking algorithms from scratch, demonstrating the power of ensemble learning in reducing variance and improving predictive performance.

- **Random Forest, AdaBoost, and Gradient Boosted Decision Trees**: Developed these powerful ensemble methods, emphasizing their application to a variety of machine learning tasks. 

## Part 5: Kernel Density Estimation and Hidden Markov Models

- **Kernel Density Estimation (KDE)**: Implemented KDE from scratch, providing a non-parametric approach to estimating probability densities.
  
- **Hidden Markov Models (HMMs)**: Tuned HMM parameters for diverse applications, exploring their role in sequence modeling and time series analysis.
  
